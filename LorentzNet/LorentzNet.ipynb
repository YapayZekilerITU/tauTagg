{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "142336d2-9177-4c2f-b0eb-c3813564c4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fdb3ea3-8977-4c31-894f-cbd77440625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#literatürden alınan blok\n",
    "# Utility functions for LorentzNet\n",
    "def unsorted_segment_sum(data, segment_ids, num_segments):\n",
    "    result = data.new_zeros((num_segments, data.size(1)))\n",
    "    result.index_add_(0, segment_ids, data)\n",
    "    return result\n",
    "\n",
    "def unsorted_segment_mean(data, segment_ids, num_segments):\n",
    "    result = data.new_zeros((num_segments, data.size(1)))\n",
    "    count = data.new_zeros((num_segments, data.size(1)))\n",
    "    result.index_add_(0, segment_ids, data)\n",
    "    count.index_add_(0, segment_ids, torch.ones_like(data))\n",
    "    return result / count.clamp(min=1)\n",
    "\n",
    "def normsq4(p):\n",
    "    psq = torch.pow(p, 2)\n",
    "    return 2 * psq[..., 0] - psq.sum(dim=-1)\n",
    "\n",
    "def dotsq4(p, q):\n",
    "    psq = p * q\n",
    "    return 2 * psq[..., 0] - psq.sum(dim=-1)\n",
    "\n",
    "def psi(p):\n",
    "    return torch.sign(p) * torch.log(torch.abs(p) + 1)\n",
    "\n",
    "# LorentzNet Building Blocks\n",
    "class LGEB(nn.Module):\n",
    "    def __init__(self, n_input, n_output, n_hidden, n_node_attr=0,\n",
    "                 dropout=0., c_weight=1.0, last_layer=False):\n",
    "        super(LGEB, self).__init__()\n",
    "        self.c_weight = c_weight\n",
    "        n_edge_attr = 2  # dims for Minkowski norm & inner product\n",
    "\n",
    "        self.phi_e = nn.Sequential(\n",
    "            nn.Linear(n_input * 2 + n_edge_attr, n_hidden, bias=False),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.phi_h = nn.Sequential(\n",
    "            nn.Linear(n_hidden + n_input + n_node_attr, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_output))\n",
    "\n",
    "        layer = nn.Linear(n_hidden, 1, bias=False)\n",
    "        torch.nn.init.xavier_uniform_(layer.weight, gain=0.001)\n",
    "\n",
    "        self.phi_x = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            layer)\n",
    "\n",
    "        self.phi_m = nn.Sequential(\n",
    "            nn.Linear(n_hidden, 1),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "        self.last_layer = last_layer\n",
    "        if last_layer:\n",
    "            del self.phi_x\n",
    "\n",
    "    def m_model(self, hi, hj, norms, dots):\n",
    "        out = torch.cat([hi, hj, norms, dots], dim=1)\n",
    "        out = self.phi_e(out)\n",
    "        w = self.phi_m(out)\n",
    "        out = out * w\n",
    "        return out\n",
    "\n",
    "    def h_model(self, h, edges, m, node_attr):\n",
    "        i, j = edges\n",
    "        agg = unsorted_segment_sum(m, i, num_segments=h.size(0))\n",
    "        agg = torch.cat([h, agg, node_attr], dim=1)\n",
    "        out = h + self.phi_h(agg)\n",
    "        return out\n",
    "\n",
    "    def x_model(self, x, edges, x_diff, m):\n",
    "        i, j = edges\n",
    "        trans = x_diff * self.phi_x(m)\n",
    "        trans = torch.clamp(trans, min=-100, max=100)\n",
    "        agg = unsorted_segment_mean(trans, i, num_segments=x.size(0))\n",
    "        x = x + agg * self.c_weight\n",
    "        return x\n",
    "\n",
    "    def minkowski_feats(self, edges, x):\n",
    "        i, j = edges\n",
    "        x_diff = x[i] - x[j]\n",
    "        norms = normsq4(x_diff).unsqueeze(1)\n",
    "        dots = dotsq4(x[i], x[j]).unsqueeze(1)\n",
    "        norms, dots = psi(norms), psi(dots)\n",
    "        return norms, dots, x_diff\n",
    "\n",
    "    def forward(self, h, x, edges, node_attr=None):\n",
    "        i, j = edges\n",
    "        norms, dots, x_diff = self.minkowski_feats(edges, x)\n",
    "\n",
    "        m = self.m_model(h[i], h[j], norms, dots)\n",
    "        if not self.last_layer:\n",
    "            x = self.x_model(x, edges, x_diff, m)\n",
    "        h = self.h_model(h, edges, m, node_attr)\n",
    "        return h, x, m\n",
    "        \n",
    "\n",
    "class LorentzNet(nn.Module):\n",
    "    def __init__(self, n_scalar, n_hidden, n_class = 2, n_layers = 6, c_weight = 1e-3, dropout = 0.):\n",
    "        super(LorentzNet, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Linear(n_scalar, n_hidden)\n",
    "        self.LGEBs = nn.ModuleList([LGEB(self.n_hidden, self.n_hidden, self.n_hidden, \n",
    "                                    n_node_attr=n_scalar, dropout=dropout,\n",
    "                                    c_weight=c_weight, last_layer=(i==n_layers-1))\n",
    "                                    for i in range(n_layers)])\n",
    "        self.graph_dec = nn.Sequential(nn.Linear(self.n_hidden, self.n_hidden),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Dropout(dropout),\n",
    "                                       nn.Linear(self.n_hidden, n_class)) # classification\n",
    "\n",
    "    def forward(self, scalars, x, edges, node_mask, edge_mask, n_nodes):\n",
    "        h = self.embedding(scalars)\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            h, x, _ = self.LGEBs[i](h, x, edges, node_attr=scalars)\n",
    "\n",
    "        h = h * node_mask\n",
    "        h = h.view(-1, n_nodes, self.n_hidden)\n",
    "        h = torch.mean(h, dim=1)\n",
    "        pred = self.graph_dec(h)\n",
    "        return pred.squeeze(1)\n",
    "\n",
    "# Data preprocessing functions\n",
    "def create_edges(n_particles):\n",
    "    \"\"\"Create fully connected edges for each particle in the jet\"\"\"\n",
    "    edges = []\n",
    "    for i in range(n_particles):\n",
    "        for j in range(n_particles):\n",
    "            if i != j:\n",
    "                edges.append((i, j))\n",
    "    return torch.tensor(edges).t().contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94593d9c-951a-4907-85d1-5b242d8e04ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Device selection\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92cbb66d-063d-451b-bda4-ff8bd8f4d82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "train_data = pd.read_parquet('z_train.parquet').head(1000)\n",
    "test_data = pd.read_parquet('z_test.parquet').head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5bec5de-6c7e-4fe2-8ba2-625b536cb854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['reco_cand_p4s', 'reco_cand_charge', 'reco_cand_pdg', 'reco_jet_p4s',\n",
      "       'reco_cand_dxy', 'reco_cand_dz', 'reco_cand_dxy_err',\n",
      "       'reco_cand_dz_err', 'gen_jet_p4s', 'gen_jet_tau_decaymode',\n",
      "       'gen_jet_tau_p4s'],\n",
      "      dtype='object')\n",
      "Index(['reco_cand_p4s', 'reco_cand_charge', 'reco_cand_pdg', 'reco_jet_p4s',\n",
      "       'reco_cand_dxy', 'reco_cand_dz', 'reco_cand_dxy_err',\n",
      "       'reco_cand_dz_err', 'gen_jet_p4s', 'gen_jet_tau_decaymode',\n",
      "       'gen_jet_tau_p4s'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_data.columns)\n",
    "print(test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29358330-c117-456a-b7af-8782df079140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             x          y           z       tau\n",
      "0    -1.480275   9.609611    3.717498  0.347857\n",
      "1     7.567224 -12.299459   19.494498  3.541984\n",
      "2     3.145566  25.068748  -18.013098  0.139616\n",
      "3    15.081155 -19.199248  -35.699724  2.340866\n",
      "4   -67.428568 -36.805377   13.092938  0.797328\n",
      "..         ...        ...         ...       ...\n",
      "995  -8.098647 -16.194725  123.337807  0.534220\n",
      "996  -6.845994  27.920566  -36.060777  7.719397\n",
      "997  25.887461   5.285017   48.852423  0.953013\n",
      "998 -10.641444  34.619491  -21.856480  1.314122\n",
      "999  33.227768   2.197429  -43.691699  6.642142\n",
      "\n",
      "[1000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# 4momenta özelliklerini çıkarma\n",
    "train_data_reco_jet_df = pd.DataFrame(train_data['reco_jet_p4s'].tolist())\n",
    "test_data_reco_jet_df = pd.DataFrame(test_data['reco_jet_p4s'].tolist())\n",
    "print(train_data_reco_jet_df)\n",
    "train_data_reco_jet_df = train_data_reco_jet_df.to_numpy()\n",
    "test_data_reco_jet_df = test_data_reco_jet_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd38d59b-be01-4fcd-b5c5-477886cb98d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0      1      2     3     4      5     6     7     8   9   ...  22  23  \\\n",
      "0    211   22.0   22.0   NaN   NaN    NaN   NaN   NaN   NaN NaN  ... NaN NaN   \n",
      "1    211   22.0  130.0  22.0   NaN    NaN   NaN   NaN   NaN NaN  ... NaN NaN   \n",
      "2    211    NaN    NaN   NaN   NaN    NaN   NaN   NaN   NaN NaN  ... NaN NaN   \n",
      "3    211   22.0   22.0  22.0   NaN    NaN   NaN   NaN   NaN NaN  ... NaN NaN   \n",
      "4    211   22.0   22.0   NaN   NaN    NaN   NaN   NaN   NaN NaN  ... NaN NaN   \n",
      "..   ...    ...    ...   ...   ...    ...   ...   ...   ...  ..  ...  ..  ..   \n",
      "995   22  211.0    NaN   NaN   NaN    NaN   NaN   NaN   NaN NaN  ... NaN NaN   \n",
      "996  211  130.0    NaN   NaN   NaN    NaN   NaN   NaN   NaN NaN  ... NaN NaN   \n",
      "997   22   22.0  211.0  22.0   NaN    NaN   NaN   NaN   NaN NaN  ... NaN NaN   \n",
      "998  211  211.0   11.0  22.0   NaN    NaN   NaN   NaN   NaN NaN  ... NaN NaN   \n",
      "999  211   22.0   22.0  22.0  22.0  130.0  22.0  22.0  22.0 NaN  ... NaN NaN   \n",
      "\n",
      "     24  25  26  27  28  29  30  31  \n",
      "0   NaN NaN NaN NaN NaN NaN NaN NaN  \n",
      "1   NaN NaN NaN NaN NaN NaN NaN NaN  \n",
      "2   NaN NaN NaN NaN NaN NaN NaN NaN  \n",
      "3   NaN NaN NaN NaN NaN NaN NaN NaN  \n",
      "4   NaN NaN NaN NaN NaN NaN NaN NaN  \n",
      "..   ..  ..  ..  ..  ..  ..  ..  ..  \n",
      "995 NaN NaN NaN NaN NaN NaN NaN NaN  \n",
      "996 NaN NaN NaN NaN NaN NaN NaN NaN  \n",
      "997 NaN NaN NaN NaN NaN NaN NaN NaN  \n",
      "998 NaN NaN NaN NaN NaN NaN NaN NaN  \n",
      "999 NaN NaN NaN NaN NaN NaN NaN NaN  \n",
      "\n",
      "[1000 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "#PDGid özelliklerini çıkarma\n",
    "scalar_column='reco_cand_pdg'\n",
    "max_particles=1000\n",
    "train_data_pdg_df = pd.DataFrame(train_data['reco_cand_pdg'].tolist())\n",
    "test_data_pdg_df = pd.DataFrame(test_data['reco_cand_pdg'].tolist())\n",
    "print(train_data_pdg_df)\n",
    "train_data_pdg_df = train_data_pdg_df.to_numpy()\n",
    "test_data_pdg_df = test_data_pdg_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff04e7dc-ebd8-474c-9858-f41519961d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -1.4803,   9.6096,   3.7175,   0.3479],\n",
      "        [  7.5672, -12.2995,  19.4945,   3.5420],\n",
      "        [  3.1456,  25.0687, -18.0131,   0.1396],\n",
      "        ...,\n",
      "        [ 25.8875,   5.2850,  48.8524,   0.9530],\n",
      "        [-10.6414,  34.6195, -21.8565,   1.3141],\n",
      "        [ 33.2278,   2.1974, -43.6917,   6.6421]])\n",
      "tensor([[211.,  22.,  22.,  ...,  nan,  nan,  nan],\n",
      "        [211.,  22., 130.,  ...,  nan,  nan,  nan],\n",
      "        [211.,  nan,  nan,  ...,  nan,  nan,  nan],\n",
      "        ...,\n",
      "        [ 22.,  22., 211.,  ...,  nan,  nan,  nan],\n",
      "        [211., 211.,  11.,  ...,  nan,  nan,  nan],\n",
      "        [211.,  22.,  22.,  ...,  nan,  nan,  nan]])\n"
     ]
    }
   ],
   "source": [
    "#create tensor\n",
    "train_4vecs = torch.tensor(train_data_reco_jet_df, dtype=torch.float32)\n",
    "test_4vecs = torch.tensor(test_data_reco_jet_df, dtype=torch.float32)\n",
    "print(train_4vecs)\n",
    "train_scalars = torch.tensor(train_data_pdg_df, dtype=torch.float32)\n",
    "test_scalars = torch.tensor(test_data_pdg_df, dtype=torch.float32)\n",
    "print(train_scalars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afb47a06-7cb1-4635-babf-9bf6a0cc239b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "Updated train labels: [0 1 2 3 4 5 6 7 8]\n",
      "Updated test labels: [0 1 2 3 5 6 7]\n",
      "Train Scalars Shape: torch.Size([1000, 32])\n",
      "Train Vectors Shape: torch.Size([1000, 4])\n",
      "Train Masks Shape: torch.Size([1000, 1000])\n"
     ]
    }
   ],
   "source": [
    "# Padding\n",
    "train_scalars_padded = np.zeros((len(train_scalars), max_particles, train_scalars.shape[-1]))\n",
    "test_scalars_padded = np.zeros((len(test_scalars), max_particles, test_scalars.shape[-1]))\n",
    "\n",
    "for i, jet in enumerate(train_scalars):\n",
    "    jet_len = min(len(jet), max_particles)\n",
    "    train_scalars_padded[i, :jet_len, :] = jet[:jet_len]\n",
    "    \n",
    "for i, jet in enumerate(test_scalars):\n",
    "    jet_len = min(len(jet), max_particles)\n",
    "    test_scalars_padded[i, :jet_len, :] = jet[:jet_len]\n",
    "    \n",
    "# Create masks\n",
    "train_mask = (train_scalars_padded.sum(axis=-1) != 0).astype(np.float32)\n",
    "test_mask = (test_scalars_padded.sum(axis=-1) != 0).astype(np.float32)\n",
    "print(train_mask)\n",
    "\n",
    "train_masks = torch.tensor(train_mask, dtype=torch.float32)\n",
    "test_masks = torch.tensor(test_mask, dtype=torch.float32)\n",
    "\n",
    "#Label\n",
    "l_train = train_data['gen_jet_tau_decaymode'].values\n",
    "l_test = test_data['gen_jet_tau_decaymode'].values\n",
    "\n",
    "# Remap labels\n",
    "all_labels = np.unique(np.concatenate([l_train, l_test]))\n",
    "label_map = {old: new for new, old in enumerate(all_labels)}\n",
    "num_classes = len(all_labels)\n",
    "\n",
    "l_train = np.array([label_map[label] for label in l_train])\n",
    "l_test = np.array([label_map[label] for label in l_test])\n",
    "print(\"Updated train labels:\", np.unique(l_train))\n",
    "print(\"Updated test labels:\", np.unique(l_test))\n",
    "\n",
    "print(\"Train Scalars Shape:\", train_scalars.shape)\n",
    "print(\"Train Vectors Shape:\", train_4vecs.shape)\n",
    "print(\"Train Masks Shape:\", train_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9f1d5d5-6ffe-442f-9bbd-9661f80b7111",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_edge_mask' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 24\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscalars[idx], \n\u001b[0;32m     16\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectors[idx],\n\u001b[0;32m     17\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medges,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_particles,\n\u001b[0;32m     21\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx])\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Create datasets and dataloaders\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m JetDataset(train_scalars, train_4vecs, l_train, train_masks)\n\u001b[0;32m     25\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m JetDataset(test_scalars, test_4vecs, l_test, test_masks)\n\u001b[0;32m     27\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m, in \u001b[0;36mJetDataset.__init__\u001b[1;34m(self, scalars, vectors, labels, masks, n_particles)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasks \u001b[38;5;241m=\u001b[39m masks\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medges \u001b[38;5;241m=\u001b[39m create_edges(n_particles)\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_mask \u001b[38;5;241m=\u001b[39m create_edge_mask(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medges)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_particles \u001b[38;5;241m=\u001b[39m n_particles\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_edge_mask' is not defined"
     ]
    }
   ],
   "source": [
    "class JetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, scalars, vectors, labels, masks, n_particles=1000):\n",
    "        self.scalars = scalars\n",
    "        self.vectors = vectors\n",
    "        self.labels = labels\n",
    "        self.masks = masks\n",
    "        self.edges = create_edges(n_particles)\n",
    "        self.edge_mask = create_edge_mask(self.edges)\n",
    "        self.n_particles = n_particles\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.scalars[idx], \n",
    "                self.vectors[idx],\n",
    "                self.edges,\n",
    "                self.masks[idx],\n",
    "                self.edge_mask,\n",
    "                self.n_particles,\n",
    "                self.labels[idx])\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = JetDataset(train_scalars, train_4vecs, l_train, train_masks)\n",
    "test_dataset = JetDataset(test_scalars, test_4vecs, l_test, test_masks)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a3ceb5-859a-4ddf-925f-413fe0c9a390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = LorentzNet(\n",
    "    n_scalar=train_scalars.shape[-1],\n",
    "    n_hidden=64,\n",
    "    n_class=num_classes,\n",
    "    n_layers=6,\n",
    "    c_weight=1e-3,\n",
    "    dropout=0.1\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc08193-53eb-460e-b130-5aa837adef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "n_epochs = 10\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc='Training'):\n",
    "        scalars, vectors, edges, node_mask, edge_mask, n_particles, labels = [b.to(device) for b in batch]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(scalars, vectors, edges, node_mask, edge_mask, n_particles)\n",
    "        print(outputs.shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(train_loader), 100. * correct / total\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            scalars, vectors, edges, node_mask, edge_mask, n_particles, labels = [b.to(device) for b in batch]\n",
    "            \n",
    "            outputs = model(scalars, vectors, edges, node_mask, edge_mask, n_particles)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(test_loader), 100. * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84d0b36-1db0-4ad4-a17f-2d8297ddf544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "n_epochs = 100\n",
    "best_acc = 0\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "        \n",
    "    print(f'Epoch: {epoch+1}/{n_epochs}')\n",
    "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        \n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(\"Best model saved!\")\n",
    "    \n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf9c05e-c0f4-4819-a73c-b28b413fd989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Evaluation\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "print(f'Final Test Loss: {test_loss:.4f}, Final Test Acc: {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5ea742-c97c-4a06-91b8-7e0db522fd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "y_true = []\n",
    "y_pred = []\n",
    "    \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        scalars, vectors, edges, node_mask, edge_mask, n_particles, labels = [b.to(device) for b in batch]\n",
    "        outputs = model(scalars, vectors, edges, node_mask, edge_mask, n_particles)\n",
    "        _, predicted = outputs.max(1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "    \n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=range(num_classes), yticklabels=range(num_classes))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7797db-84dc-43d4-8710-87972d08af7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC Curve\n",
    "y_true_bin = label_binarize(y_true, classes=range(num_classes))\n",
    "y_pred_scores = []\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        scalars, vectors, edges, node_mask, edge_mask, n_particles, labels = [b.to(device) for b in batch]\n",
    "        outputs = model(scalars, vectors, edges, node_mask, edge_mask, n_particles)\n",
    "        y_pred_scores.extend(F.softmax(outputs, dim=1).cpu().numpy())\n",
    "    \n",
    "y_pred_scores = np.array(y_pred_scores)\n",
    "    \n",
    "for i in range(num_classes):\n",
    "    fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_pred_scores[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc:.2f})')\n",
    "    \n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
